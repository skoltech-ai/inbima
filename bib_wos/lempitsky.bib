
@inproceedings{ WOS:000693397600081,
Author = {Bashirov, Renat and Ianina, Anastasia and Iskakov, Karim and Kononenko,
   Yevgeniy and Strizhkova, Valeriya and Lempitsky, Victor and Vakhitov,
   Alexander},
Book-Group-Author = {{IEEE}},
Title = {{Real-time RGBD-based Extended Body Pose Estimation}},
Booktitle = {{2021 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION WACV 2021}},
Series = {{IEEE Winter Conference on Applications of Computer Vision}},
Year = {{2021}},
Pages = {{2806-2815}},
Note = {{IEEE Winter Conference on Applications of Computer Vision (WACV), ELECTR
   NETWORK, JAN 05-09, 2021}},
Organization = {{IEEE; IEEE Comp Soc; Adobe; Amazon; iRobot; Kitware; Verisk}},
Abstract = {{We present a system for real-time RGBD-based estimation of 3D human
   pose. We use parametric 3D deformable human mesh model (SMPL-X) as a
   representation and focus on the real-time estimation of parameters for
   the body pose, hands pose and facial expression from Kinect Azure RGB-D
   camera. We train estimators of body pose and facial expression
   parameters. Both estimators use previously published landmark extractors
   as input and custom annotated datasets for supervision, while hand pose
   is estimated directly by a previously published method. We combine the
   predictions of those estimators into a temporally-smooth human pose. We
   train the facial expression extractor on a large talking face dataset,
   which we annotate with facial expression parameters. For the body pose
   we collect and annotate a dataset of 56 people captured from a rig of 5
   Kinect Azure RGB-D cameras and use it together with a large motion
   capture AMASS dataset. Our RGB-D body pose model outperforms the
   state-of-the-art RGB-only methods and works on the same level of
   accuracy compared to a slower RGB-D optimization-based solution. The
   combined system runs at 25 FPS on a server with a single GPU. The code
   will be available at saic-violet.github.io/rgbd-kinect-pose}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Bashirov, R (Corresponding Author), Samsung AI Ctr, Moscow, Russia.
   Bashirov, Renat; Ianina, Anastasia; Iskakov, Karim; Kononenko, Yevgeniy; Lempitsky, Victor; Vakhitov, Alexander, Samsung AI Ctr, Moscow, Russia.
   Iskakov, Karim; Strizhkova, Valeriya; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1109/WACV48630.2021.00285}},
ISSN = {{2472-6737}},
ISBN = {{978-0-7381-4266-1}},
Research-Areas = {{Computer Science; Engineering; Imaging Science \& Photographic
   Technology}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Imaging Science \& Photographic Technology}},
Number-of-Cited-References = {{48}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BS1MO}},
Unique-ID = {{WOS:000693397600081}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@article{ WOS:000522423700001,
Author = {Jones, Fiona M. and Arteta, Carlos and Zisserman, Andrew and Lempitsky,
   Victor and Lintott, Chris J. and Hart, Tom},
Title = {{Processing citizen science- and machine-annotated time-lapse imagery for
   biologically meaningful metrics}},
Journal = {{SCIENTIFIC DATA}},
Year = {{2020}},
Volume = {{7}},
Number = {{1}},
Month = {{MAR 26}},
Abstract = {{Time-lapse cameras facilitate remote and high-resolution monitoring of
   wild animal and plant communities, but the image data produced require
   further processing to be useful. Here we publish pipelines to process
   raw time-lapse imagery, resulting in count data (number of penguins per
   image) and `nearest neighbour distance' measurements. The latter provide
   useful summaries of colony spatial structure (which can indicate
   phenological stage) and can be used to detect movement - metrics which
   could be valuable for a number of different monitoring scenarios,
   including image capture during aerial surveys. We present two
   alternative pathways for producing counts: (1) via the Zooniverse
   citizen science project Penguin Watch and (2) via a computer vision
   algorithm (Pengbot), and share a comparison of citizen science-, machine
   learning-, and expert- derived counts. We provide example files for 14
   Penguin Watch cameras, generated from 63,070 raw images annotated by
   50,445 volunteers. We encourage the use of this large open-source
   dataset, and the associated processing methodologies, for both
   ecological studies and continued machine learning and computer vision
   development.
   Measurement(s)number of penguins per image center dot nearest neighbour
   distance center dot movement of individuals in imagesTechnology
   Type(s)machine learningFactor Type(s)geographic location center dot time
   of data collectionSample Characteristic - OrganismPygoscelis papua
   center dot Pygoscelis antarcticus center dot Pygoscelis adeliaeSample
   Characteristic - LocationAntarctic Peninsula center dot South Shetland
   Islands center dot South Georgia and South Sandwich Islands
   Machine-accessible metadata file describing the reported data:
   10.6084/m9.figshare.11807325}},
Publisher = {{NATURE PUBLISHING GROUP}},
Address = {{MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND}},
Type = {{Article; Data Paper}},
Language = {{English}},
Affiliation = {{Jones, FM; Hart, T (Corresponding Author), Univ Oxford, Dept Zool, 11a Mansfield Rd, Oxford OX1 3SZ, England.
   Jones, Fiona M.; Hart, Tom, Univ Oxford, Dept Zool, 11a Mansfield Rd, Oxford OX1 3SZ, England.
   Arteta, Carlos; Zisserman, Andrew, Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.
   Lempitsky, Victor, Samsung AI Ctr, Butyrskiy Val Ulitsa 10, Moscow 125047, Russia.
   Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Bolshoy Blvd 30,Bld 1, Moscow 121205, Russia.
   Lintott, Chris J., Univ Oxford, Zooniverse, Dept Phys, Denys Wilkinson Bldg,Keble Rd, Oxford OX1 3RH, England.}},
DOI = {{10.1038/s41597-020-0442-6}},
Article-Number = {{102}},
EISSN = {{2052-4463}},
Keywords-Plus = {{CLIMATE-CHANGE}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{fiona.jones@zoo.ox.ac.uk
   tom.hart@zoo.ox.ac.uk}},
ORCID-Numbers = {{Zisserman, Andrew/0000-0002-8945-8573}},
Funding-Acknowledgement = {{Alfred P. Sloan FoundationAlfred P. Sloan Foundation; Google Global
   Impact AwardGoogle Incorporated; NERCUK Research \& Innovation
   (UKRI)Natural Environment Research Council (NERC)}},
Funding-Text = {{The authors wish to thank the Zooniverse team
   (https://www.zooniverse.org/about/team), Penguin Watch partners and
   collaborators
   (https://www.zooniverse.org/projects/penguintom79/penguin-watch/about/te
   am), and citizen science volunteers. We would also like to thank J.
   Arthur, H.R. Torsey and Z. Rockaiova (nee Machackova) for moderating the
   Penguin Watch `Talk' forum. Zooniverse and Penguin Watch were supported
   by grants from the Alfred P. Sloan Foundation and a Google Global Impact
   Award. This work has been supported by NERC and individual donors to the
   Penguin Watch project. Finally, we express our gratitude to Quark
   Expeditions, Cheesemans' Ecology Safaris and Oceanwide Expeditions for
   their collaboration during fieldwork. Fiona Jones and Tom Hart had full
   access to all the data in the study and take responsibility for the
   integrity of the data and the accuracy of the data analysis.}},
Number-of-Cited-References = {{22}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{Sci. Data}},
Doc-Delivery-Number = {{KY2SX}},
Unique-ID = {{WOS:000522423700001}},
OA = {{gold, Green Published}},
DA = {{2021-10-27}},
}

@article{ WOS:000518246900004,
Author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
Title = {{Deep Image Prior}},
Journal = {{INTERNATIONAL JOURNAL OF COMPUTER VISION}},
Year = {{2020}},
Volume = {{128}},
Number = {{7}},
Pages = {{1867-1888}},
Month = {{JUL}},
Abstract = {{Deep convolutional networks have become a popular tool for image
   generation and restoration. Generally, their excellent performance is
   imputed to their ability to learn realistic image priors from a large
   number of example images. In this paper, we show that, on the contrary,
   the structure of a generator network is sufficient to capture a great
   deal of low-level image statistics prior to any learning. In order to do
   so, we show that a randomly-initialized neural network can be used as a
   handcrafted prior with excellent results in standard inverse problems
   such as denoising, super-resolution, and inpainting. Furthermore, the
   same prior can be used to invert deep neural representations to diagnose
   them, and to restore images based on flash-no flash input pairs. Apart
   from its diverse applications, our approach highlights the inductive
   bias captured by standard generator network architectures. It also
   bridges the gap between two very popular families of image restoration
   methods: learning-based methods using deep convolutional networks and
   learning-free methods based on handcrafted image priors such as
   self-similarity (Code and supplementary material are available at ).}},
Publisher = {{SPRINGER}},
Address = {{VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ulyanov, D (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Ulyanov, Dmitry; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Vedaldi, Andrea, Univ Oxford, Oxford, England.}},
DOI = {{10.1007/s11263-020-01303-4}},
Early Access Date = {{MAR 2020}},
ISSN = {{0920-5691}},
EISSN = {{1573-1405}},
Keywords = {{Convolutional networks; Generative deep networks; Inverse problems;
   Image restoration; Image superresolution; Image denoising; Natural image
   prior}},
Keywords-Plus = {{SCALE-SPACE METHODS}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{dmitry.ulyanov@skoltech.ru
   vedaldi@robots.ox.ac.uk
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Ministry of Education and Science of the Russian FederationMinistry of
   Education and Science, Russian Federation {[}14.756.31.0001];
   ERCEuropean Research Council (ERC)European Commission {[}638009-IDIU]}},
Funding-Text = {{DU and VL are supported by the Ministry of Education and Science of the
   Russian Federation (Grant 14.756.31.0001) and AV is supported by ERC
   638009-IDIU.}},
Number-of-Cited-References = {{64}},
Times-Cited = {{23}},
Usage-Count-Last-180-days = {{9}},
Usage-Count-Since-2013 = {{37}},
Journal-ISO = {{Int. J. Comput. Vis.}},
Doc-Delivery-Number = {{MC3NG}},
Unique-ID = {{WOS:000518246900004}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000620679506069,
Author = {Khrulkov, Valentin and Mirvakhabova, Leyla and Ustinova, Evgeniya and
   Oseledets, Ivan and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Hyperbolic Image Embeddings}},
Booktitle = {{2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2020}},
Pages = {{6417-6427}},
Note = {{IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
   ELECTR NETWORK, JUN 14-19, 2020}},
Organization = {{IEEE; CVF}},
Abstract = {{Computer vision tasks such as image classification, image retrieval, and
   few-shot learning are currently dominated by Euclidean and spherical
   embeddings so that the final decisions about class belongings or the
   degree of similarity are made using linear hyperplanes, Euclidean
   distances, or spherical geodesic distances (cosine similarity). In this
   work, we demonstrate that in many practical scenarios, hyperbolic
   embeddings provide a better alternative.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Khrulkov, V (Corresponding Author), Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.
   Khrulkov, V (Corresponding Author), Yandex, Moscow, Russia.
   Khrulkov, Valentin; Mirvakhabova, Leyla; Ustinova, Evgeniya; Oseledets, Ivan; Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.
   Oseledets, Ivan, Russian Acad Sci, Inst Numer Math, Moscow, Russia.
   Lempitsky, Victor, Samsung AI Ctr, Moscow, Russia.
   Khrulkov, Valentin, Yandex, Moscow, Russia.}},
DOI = {{10.1109/CVPR42600.2020.00645}},
ISSN = {{1063-6919}},
ISBN = {{978-1-7281-7168-5}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{valentin.khrulkov@skoltech.ru
   leyla.mirvakhabova@skoltech.ru
   evgeniya.ustinova@skoltech.ru
   i.oseledets@skoltech.ru
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Ministry of Science and Education of Russian FederationMinistry of
   Education and Science, Russian Federation {[}14.756.31.000]}},
Funding-Text = {{This work was funded by the Ministry of Science and Education of Russian
   Federation as a part of Mega Grant Research Project 14.756.31.000.}},
Number-of-Cited-References = {{61}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BQ8LD}},
Unique-ID = {{WOS:000620679506069}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000653085200118,
Author = {Kolos, Maria and Sevastopolsky, Artem and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{TRANSPR: Transparency Ray-Accumulating Neural 3D Scene Point Renderer}},
Booktitle = {{2020 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2020)}},
Series = {{International Conference on 3D Vision}},
Year = {{2020}},
Pages = {{1167-1175}},
Note = {{8th International Conference on 3D Vision (3DV), ELECTR NETWORK, NOV
   25-28, 2020}},
Organization = {{Facebook Real Labs; Microsoft; Huawei; Ocado Technol; Omron; Cyber Agent
   AI Diu; Denso IT Lab; Google; Sensetime; Tencent AI Lab; Toppan; Apple}},
Abstract = {{We propose and evaluate a neural point-based graphics method that can
   model semi-transparent scene parts. Similarly to its predecessor
   pipeline, ours uses point clouds to model proxy geometry, and augments
   each point with a neural descriptor. Additionally, a learnable
   transparency value is introduced in our approach for each point.
   Our neural rendering procedure consists of two steps. Firstly, the point
   cloud is rasterized using ray marching into a multi-channel image. This
   is followed by the neural rendering step that ``translates{''} the
   rasterized image into an RGB output using a learnable convolutional
   network. New scenes can be modeled using gradient-based optimization of
   neural descriptors and of the rendering network.
   We show that novel views of semi-transparent point cloud scenes can be
   generated after training with our approach. Our experiments demonstrate
   the benefit of introducing semi-transparency into the neural point-based
   modeling for a range of scenes with semi-transparent parts.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Kolos, M (Corresponding Author), Samsung AI Ctr, Moscow, Russia.
   Kolos, Maria; Sevastopolsky, Artem; Lempitsky, Victor, Samsung AI Ctr, Moscow, Russia.
   Sevastopolsky, Artem; Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.}},
DOI = {{10.1109/3DV50981.2020.00127}},
ISSN = {{2378-3826}},
EISSN = {{2475-7888}},
ISBN = {{978-1-7281-8128-8}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Electrical \& Electronic}},
Author-Email = {{mariakolos1@gmail.com
   a.sevastopol@samsung.com
   v.lempitsky@samsung.com}},
Number-of-Cited-References = {{19}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BR4SK}},
Unique-ID = {{WOS:000653085200118}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000620679504012,
Author = {Kulikov, Victor and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Instance Segmentation of Biological Images Using Harmonic Embeddings}},
Booktitle = {{2020 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2020}},
Pages = {{3842-3850}},
Note = {{IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
   ELECTR NETWORK, JUN 14-19, 2020}},
Organization = {{IEEE; CVF}},
Abstract = {{We present a new instance segmentation approach tailored to biological
   images, where instances may correspond to individual cells, organisms or
   plant parts. Unlike instance segmentation for user photographs or road
   scenes, in biological data object instances may be particularly densely
   packed, the appearance variation may be particularly low, the processing
   power may be restricted, while, on the other hand, the variability of
   sizes of individual instances may be limited. The proposed approach
   successfully addresses these peculiarities.
   Our approach describes each object instance using an expectation of a
   limited number of sine waves with frequencies and phases adjusted to
   particular object sizes and densities. At train time, a
   fully-convolutional network is learned to predict the object embeddings
   at each pixel using a simple pixelwise regression loss, while at test
   time the instances are recovered using clustering in the embedding
   space. In the experiments, we show that our approach outperforms
   previous embedding-based instance segmentation approaches on a number of
   biological datasets, achieving state-of-the-art on a popular CVPPP
   benchmark. This excellent performance is combined with computational
   efficiency that is needed for deployment to domain specialists.
   The source code of the approach is available at
   https://github.com/kulikovv/harmonic.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Kulikov, V (Corresponding Author), PicsArt Inc, Moscow, Russia.
   Kulikov, Victor, PicsArt Inc, Moscow, Russia.
   Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Samsung AI Ctr Moscow, Moscow, Russia.
   Kulikov, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1109/CVPR42600.2020.00390}},
ISSN = {{1063-6919}},
ISBN = {{978-1-7281-7168-5}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{victor.kulikov@picsart.com
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Skoltech NGP Program {[}MIT-Skoltech 1911/R]}},
Funding-Text = {{This work was supported by the Skoltech NGP Program (MIT-Skoltech
   1911/R). Computations were performed on Skoltech HPC cluster Zhores.}},
Number-of-Cited-References = {{30}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BQ8LD}},
Unique-ID = {{WOS:000620679504012}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@article{ WOS:000471040500044,
Author = {Kulikov, Victor and Guo, Syuan-Ming and Stone, Matthew and Goodman,
   Allen and Carpenter, Anne and Bathe, Mark and Lempitsky, Victor},
Title = {{DoGNet: A deep architecture for synapse detection in multiplexed
   fluorescence images}},
Journal = {{PLOS COMPUTATIONAL BIOLOGY}},
Year = {{2019}},
Volume = {{15}},
Number = {{5}},
Month = {{MAY}},
Abstract = {{Neuronal synapses transmit electrochemical signals between cells through
   the coordinated action of presynaptic vesicles, ion channels,
   scaffolding and adapter proteins, and membrane receptors. In situ
   structural characterization of numerous synaptic proteins simultaneously
   through multiplexed imaging facilitates a bottom-up approach to synapse
   classification and phenotypic description. Objective automation of
   efficient and reliable synapse detection within these datasets is
   essential for the high-throughput investigation of synaptic features.
   Convolutional neural networks can solve this generalized problem of
   synapse detection, however, these architectures require large numbers of
   training examples to optimize their thousands of parameters. We propose
   DoGNet, a neural network architecture that closes the gap between
   classical computer vision blob detectors, such as Difference of
   Gaussians (DoG) filters, and modern convolutional networks. DoGNet is
   optimized to analyze highly multiplexed microscopy data. Its small
   number of training parameters allows DoGNet to be trained with few
   examples, which facilitates its application to new datasets without
   overfitting. We evaluate the method on multiplexed fluorescence imaging
   data from both primary mouse neuronal cultures and mouse cortex tissue
   slices. We show that DoGNet outperforms convolutional networks with a
   low-to-moderate number of training examples, and DoGNet is efficiently
   transferred between datasets collected from separate research groups.
   DoGNet synapse localizations can then be used to guide the segmentation
   of individual synaptic protein locations and spatial extents, revealing
   their spatial organization and relative abundances within individual
   synapses. The source code is publicly available:
   https://github.com/kulikovv/dognet.
   Author summary Multiplexed fluorescence imaging of synaptic proteins
   facilitates high throughput investigations in neuroscience and drug
   discovery. Currently, there are several approaches to synapse detection
   using computational image processing. Unsupervised techniques rely on
   the a priori knowledge of synapse properties, such as size, intensity,
   and co-localization of synapse markers in each channel. For each
   experimental replicate, these parameters are typically tuned manually in
   order to obtain appropriate results. In contrast, supervised methods
   like modern convolutional networks require massive amounts of manually
   labeled data, and are sensitive to signal/noise ratios. As an
   alternative, here we propose DoGNet, a neural architecture that closes
   the gap between classical computer vision blob detectors, such as
   Difference of Gaussians (DoG) filters, and modern convolutional
   networks. This approach leverages the strengths of each approach,
   including automatic tuning of detection parameters, prior knowledge of
   the synaptic signal shape, and requiring only several training examples.
   Overall, DoGNet is a new tool for blob detection from multiplexed
   fluorescence images consisting of several up to dozens of fluorescence
   channels that requires minimal supervision due to its few input
   parameters. It offers the ability to capture complex dependencies
   between synaptic signals in distinct imaging planes, acting as a
   trainable frequency filter.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Kulikov, V (Corresponding Author), CDISE, Moscow, Russia.
   Kulikov, Victor; Lempitsky, Victor, CDISE, Moscow, Russia.
   Guo, Syuan-Ming; Stone, Matthew; Bathe, Mark, MIT, Dept Biol Engn, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   Goodman, Allen; Carpenter, Anne, Broad Inst MIT \& Harvard, Imaging Platform, Cambridge, MA 02142 USA.}},
DOI = {{10.1371/journal.pcbi.1007012}},
Article-Number = {{e1007012}},
ISSN = {{1553-734X}},
EISSN = {{1553-7358}},
Research-Areas = {{Biochemistry \& Molecular Biology; Mathematical \& Computational Biology}},
Web-of-Science-Categories  = {{Biochemical Research Methods; Mathematical \& Computational Biology}},
Author-Email = {{v.kulikov@skoltech.ru}},
ResearcherID-Numbers = {{Carpenter, Anne/C-4982-2008}},
ORCID-Numbers = {{Carpenter, Anne/0000-0003-1555-8261}},
Funding-Acknowledgement = {{Skoltech NGP Program {[}MIT-Skoltech 1911/R]; NSFNational Science
   Foundation (NSF) {[}PHY 1707999]; NIHUnited States Department of Health
   \& Human ServicesNational Institutes of Health (NIH) - USA
   {[}U01-MH106011, R01-MH112694, R35 GM122547]; NATIONAL INSTITUTE OF
   GENERAL MEDICAL SCIENCESUnited States Department of Health \& Human
   ServicesNational Institutes of Health (NIH) - USANIH National Institute
   of General Medical Sciences (NIGMS) {[}R35GM122547] Funding Source: NIH
   RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTHUnited States Department of
   Health \& Human ServicesNational Institutes of Health (NIH) - USANIH
   National Institute of Mental Health (NIMH) {[}U01MH106011, R01MH112694]
   Funding Source: NIH RePORTER}},
Funding-Text = {{This work was supported by the Skoltech NGP Program (MIT-Skoltech
   1911/R). Funding from NSF PHY 1707999 to Matthew Stone and Mark Bathe,
   NIH U01-MH106011 and R01-MH112694 to Syuan-Ming Guo and Mark Bathe, and
   NIH R35 GM122547 to Allen Goodman and Anne Carpenter is gratefully
   acknowledged. The funders had no role in study design, data collection
   and analysis, decision to publish, or preparation of the manuscript.}},
Number-of-Cited-References = {{41}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{2}},
Journal-ISO = {{PLoS Comput. Biol.}},
Doc-Delivery-Number = {{IC5WT}},
Unique-ID = {{WOS:000471040500044}},
OA = {{Green Published, gold, Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000542649305076,
Author = {Grigorev, Artur and Sevastopolsky, Artem and Vakhitov, Alexander and
   Lempitsky, Victor},
Book-Group-Author = {{IEEE Comp Soc}},
Title = {{Coordinate-based Texture Inpainting for Pose-Guided Human Image
   Generation}},
Booktitle = {{2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR 2019)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2019}},
Pages = {{12127-12136}},
Note = {{32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Long Beach, CA, JUN 16-20, 2019}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{We present a new deep learning approach to pose-guided resynthesis of
   human photographs. At the heart of the new approach is the estimation of
   the complete body surface texture based on a single photograph. Since
   the input photograph always observes only a part of the surface, we
   suggest a new inpainting method that completes the texture of the human
   body. Rather than working directly with colors of texture elements, the
   inpainting network estimates an appropriate source location in the input
   image for each element of the body surface. This correspondence field
   between the input image and the texture is then further warped into the
   target image coordinate frame based on the desired pose, effectively
   establishing the correspondence between the source and the target view
   even when the pose change is drastic. The final convolutional network
   then uses the established correspondence and all other available
   information to synthesize the output image. A fully-convolutional
   architecture with deformable skip connections guided by the estimated
   correspondence field is used. We show state-of-the-art resultfor
   pose-guided image synthesis. Additionally, we demonstrate the
   performance of our system for garment transfer and pose-guided face
   resynthesis.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Grigorev, A (Corresponding Author), Samsung AI Ctr, Moscow, Russia.
   Grigorev, A (Corresponding Author), Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.
   Grigorev, Artur; Sevastopolsky, Artem; Vakhitov, Alexander; Lempitsky, Victor, Samsung AI Ctr, Moscow, Russia.
   Grigorev, Artur; Sevastopolsky, Artem; Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.}},
DOI = {{10.1109/CVPR.2019.01241}},
ISSN = {{1063-6919}},
ISBN = {{978-1-7281-3293-8}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods}},
Author-Email = {{a.grigorev@samsung.com
   a.sevastopol@samsung.com
   a.vakhitov@samsung.com
   v.lempitsky@samsung.com}},
Number-of-Cited-References = {{38}},
Times-Cited = {{10}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{6}},
Doc-Delivery-Number = {{BP2IB}},
Unique-ID = {{WOS:000542649305076}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000548549202081,
Author = {Iskakov, Karim and Burkov, Egor and Lempitsky, Victor and Malkov, Yury},
Book-Group-Author = {{IEEE}},
Title = {{Learnable Triangulation of Human Pose}},
Booktitle = {{2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2019)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2019}},
Pages = {{7717-7726}},
Note = {{IEEE/CVF International Conference on Computer Vision (ICCV), Seoul,
   SOUTH KOREA, OCT 27-NOV 02, 2019}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{We present two novel solutions for multi-view 3D human pose estimation
   based on new learnable triangulation methods that combine 3D information
   from multiple 2D views. The first (baseline) solution is a basic
   differentiable algebraic triangulation with an addition of confidence
   weights estimated from the input images. The second solution is based on
   a novel method of volumetric aggregation from intermediate 2D backbone
   feature maps. The aggregated volume is then refined via 3D convolutions
   that produce final 3D joint heatmaps and allow implicit modelling a
   human pose prior. Crucially, both approaches are end-to-end
   differentiable, which allows us to directly optimize the target metric.
   We demonstrate transferability of the solutions across datasets and
   considerably improve the multiview state of the art on the Human3.6M
   dataset. Video demonstration, annotations and additional materials will
   be posted on our project page(1).}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Iskakov, K (Corresponding Author), Samsung AI Ctr, Moscow, Russia.
   Iskakov, Karim; Burkov, Egor; Lempitsky, Victor; Malkov, Yury, Samsung AI Ctr, Moscow, Russia.
   Burkov, Egor; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1109/ICCV.2019.00781}},
ISSN = {{1550-5499}},
ISBN = {{978-1-7281-4803-8}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{k.iskakov@samsung.com
   e.burkov@samsung.com
   v.lempitsky@samsung.com
   y.malkov@samsung.com}},
Number-of-Cited-References = {{27}},
Times-Cited = {{38}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BP3PR}},
Unique-ID = {{WOS:000548549202081}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000529484002056,
Author = {Shysheya, Aliaksandra and Zakharov, Egor and Aliev, Kara-Ali and
   Bashirov, Renat and Burkov, Egor and Iskakov, Karim and Ivakhnenko,
   Aleksei and Malkov, Yury and Pasechnik, Igor and Ulyanov, Dmitry and
   Vakhitov, Alexander and Lempitsky, Victor},
Book-Group-Author = {{IEEE Comp Soc}},
Title = {{Textured Neural Avatars}},
Booktitle = {{2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR 2019)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2019}},
Pages = {{2382-2392}},
Note = {{32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Long Beach, CA, JUN 16-20, 2019}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{We present a system for learning full body neural avatars, i.e. deep
   networks that produce full body renderings of a person for varying body
   pose and varying camera pose. Our system takes the middle path between
   the classical graphics pipeline and the recent deep learning approaches
   that generate images of humans using image-to-image translation. In
   particular, our system estimates an explicit two-dimensional texture map
   of the model surface. At the same time, it abstains from explicit shape
   modeling in 3D. Instead, at test time, the system uses a
   fully-convolutional network to directly map the configuration of body
   feature points w.r.t. the camera to the 2D texture coordinates of
   individual pixels in the image frame. We show that such system is
   capable of learning to generate realistic renderings while being trained
   on videos annotated with 3D poses and foreground masks. We also
   demonstrate that maintaining an explicit texture representation helps
   our system to achieve better generalization compared to systems that use
   direct image-to-image translation.}},
Publisher = {{IEEE COMPUTER SOC}},
Address = {{10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Shysheya, A (Corresponding Author), Samsung AI Ctr, Moscow, Russia.
   Shysheya, A (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Shysheya, Aliaksandra; Zakharov, Egor; Aliev, Kara-Ali; Bashirov, Renat; Burkov, Egor; Iskakov, Karim; Ivakhnenko, Aleksei; Malkov, Yury; Pasechnik, Igor; Ulyanov, Dmitry; Vakhitov, Alexander; Lempitsky, Victor, Samsung AI Ctr, Moscow, Russia.
   Shysheya, Aliaksandra; Zakharov, Egor; Burkov, Egor; Ulyanov, Dmitry; Vakhitov, Alexander; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1109/CVPR.2019.00249}},
ISSN = {{1063-6919}},
ISBN = {{978-1-7281-3293-8}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods}},
Number-of-Cited-References = {{74}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BO8XD}},
Unique-ID = {{WOS:000529484002056}},
DA = {{2021-10-27}},
}

@article{ WOS:000463650900001,
Author = {Vakhitov, Alexander and Lempitsky, Victor},
Title = {{Learnable Line Segment Descriptor for Visual SLAM}},
Journal = {{IEEE ACCESS}},
Year = {{2019}},
Volume = {{7}},
Pages = {{39923-39934}},
Abstract = {{Traditionally, the indirect visual motion estimation and simultaneous
   localization and mapping (SLAM) systems were based on point features. In
   recent years, several SLAM systems that use lines as primitives were
   suggested. Despite the extra robustness and accuracy brought by the line
   segment matching, the line segment descriptors used in such systems were
   hand-crafted, and therefore sub-optimal. In this paper, we suggest
   applying descriptor learning to construct line segment descriptors
   optimized for matching tasks. We show how such descriptors can be
   constructed on top of a deep yet lightweight fully-convolutional neural
   network. The coefficients of this network are trained using an
   automatically collected dataset of matching and non-matching line
   segments. The use of the fully-convolutional network ensures that the
   bulk of the computations needed to compute descriptors is shared among
   the multiple line segments in the same image, enabling efficient
   implementation. We show that the learned line segment descriptors
   outperform the previously suggested hand-crafted line segment
   descriptors both in isolation (i.e., for the subtask of distinguishing
   matching and non-matching line segments), but also when built into the
   SLAM system. We construct a new line based SLAM pipeline built upon a
   state-of-the-art point-only system. We demonstrate generalization of the
   learned parameters of the descriptor network between two well-known
   datasets for autonomous driving and indoor micro aerial vehicle
   navigation.}},
Publisher = {{IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC}},
Address = {{445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Vakhitov, A (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow 121205, Russia.
   Vakhitov, Alexander, Skolkovo Inst Sci \& Technol, Moscow 121205, Russia.
   Samsung AI Ctr, Moscow, Russia.}},
DOI = {{10.1109/ACCESS.2019.2901584}},
ISSN = {{2169-3536}},
Keywords = {{Feature descriptors; line segments; robotic perception; simultaneous
   localization and mapping; SLAM}},
Keywords-Plus = {{GEOMETRY}},
Research-Areas = {{Computer Science; Engineering; Telecommunications}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications}},
Author-Email = {{alexander.vakhitov@gmail.com}},
ResearcherID-Numbers = {{Solodushenkova, Anastasia/H-1732-2019}},
Funding-Acknowledgement = {{Russian MES Grant {[}RFMEFI61516X0003]}},
Funding-Text = {{This work was supported by the Russian MES Grant RFMEFI61516X0003.}},
Number-of-Cited-References = {{55}},
Times-Cited = {{12}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{10}},
Journal-ISO = {{IEEE Access}},
Doc-Delivery-Number = {{HS1VS}},
Unique-ID = {{WOS:000463650900001}},
OA = {{gold}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000548549204058,
Author = {Zakharov, Egor and Shysheya, Aliaksandra and Burkov, Egor and Lempitsky,
   Victor},
Book-Group-Author = {{IEEE}},
Title = {{Few-Shot Adversarial Learning of Realistic Neural Talking Head Models}},
Booktitle = {{2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2019)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2019}},
Pages = {{9458-9467}},
Note = {{IEEE/CVF International Conference on Computer Vision (ICCV), Seoul,
   SOUTH KOREA, OCT 27-NOV 02, 2019}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{Several recent works have shown how highly realistic human head images
   can be obtained by training convolutional neural networks to generate
   them. In order to create a personalized talking head model, these works
   require training on a large dataset of images of a single person.
   However, in many practical scenarios, such personalized talking head
   models need to be learned from a few image views of a person,
   potentially even a single image. Here, we present a system with such
   few-shot capability. It performs lengthy meta-learning on a large
   dataset of videos, and after that is able to frame few- and one-shot
   learning of neural talking head models of previously unseen people as
   adversarial training problems with high capacity generators and
   discriminators. Crucially, the system is able to initialize the
   parameters of both the generator and the discriminator in a
   person-specific way, so that training can be based on just a few images
   and done quickly, despite the need to tune tens of millions of
   parameters. We show that such an approach is able to learn highly
   realistic and personalized talking head models of new people and even
   portrait paintings.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Zakharov, E (Corresponding Author), Samsung AI Ctr, Moscow, Russia.
   Zakharov, E (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Zakharov, Egor; Shysheya, Aliaksandra; Burkov, Egor; Lempitsky, Victor, Samsung AI Ctr, Moscow, Russia.
   Zakharov, Egor; Shysheya, Aliaksandra; Burkov, Egor; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1109/ICCV.2019.00955}},
ISSN = {{1550-5499}},
ISBN = {{978-1-7281-4803-8}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Number-of-Cited-References = {{43}},
Times-Cited = {{45}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{4}},
Doc-Delivery-Number = {{BP3PR}},
Unique-ID = {{WOS:000548549204058}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@article{ WOS:000456088000001,
Author = {Cichocki, A. and Poggio, T. and Osowski, S. and Lempitsky, V.},
Title = {{Deep Learning: Theory and Practice}},
Journal = {{BULLETIN OF THE POLISH ACADEMY OF SCIENCES-TECHNICAL SCIENCES}},
Year = {{2018}},
Volume = {{66}},
Number = {{6}},
Month = {{DEC}},
Publisher = {{POLSKA AKAD NAUK, POLISH ACAD SCI, DIV IV TECHNICAL SCIENCES PAS}},
Address = {{PL DEFILAD 1, WARSZAWA, 00-901, POLAND}},
Type = {{Editorial Material}},
Language = {{English}},
Affiliation = {{Cichocki, A (Corresponding Author), Erlangen Nurnberg Univ, Erlangen, Germany.
   Cichocki, A., Erlangen Nurnberg Univ, Erlangen, Germany.
   Cichocki, A., RIKEN, Brain Sci Inst, Wako, Saitama, Japan.
   Cichocki, A., RIKEN, Brain Sci Inst, Lab Adv Brain Signal Proc, Wako, Saitama, Japan.
   Osowski, S., Warsaw Univ Technol, Inst Theory Elect Engn \& Elect Measurements, Elect Engn, Warsaw, Poland.
   Osowski, S., Mil Univ Technol, Warsaw, Poland.
   Lempitsky, V., Skolkovo Inst Sci \& Technol, Comp Vis Grp, Moscow, Moscow Oblast, Russia.
   Poggio, T., MIT, Dept Brain \& Cognit Sci, Cambridge, MA 02139 USA.
   Poggio, T., MIT, New NSF Ctr Brains Minds \& Machines, Cambridge, MA 02139 USA.
   Poggio, T., MIT, Comp Sci \& Artificial Intelligence Lab, Cambridge, MA 02139 USA.
   Poggio, T., MIT, McGovern Brain Inst, Cambridge, MA 02139 USA.
   Poggio, T., Amer Acad Arts \& Sci, Cambridge, England.
   Poggio, T., AAAI, Menlo Pk, CA USA.
   Poggio, T., Thinking Machines Corp, Cambridge, MA USA.
   Poggio, T., PHZ Capital Partners Inc, Phoenix, AZ USA.}},
DOI = {{10.24425/bpas.2018.125923}},
ISSN = {{0239-7528}},
EISSN = {{2300-1917}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Multidisciplinary}},
Author-Email = {{a.cichocki@sklotech.ru
   tp@csail.mit.edu
   sto@iem.pw.edu.pl
   lempitsky@skoltech.ru}},
ResearcherID-Numbers = {{Cichocki, Andrzej/AAI-4209-2020
   Osowski, Stanislaw/AAL-8713-2021
   Osowski, Stanislaw/K-1920-2018}},
ORCID-Numbers = {{Osowski, Stanislaw/0000-0003-3194-4656}},
Number-of-Cited-References = {{12}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{4}},
Journal-ISO = {{Bull. Pol. Acad. Sci.-Tech. Sci.}},
Doc-Delivery-Number = {{HH9VG}},
Unique-ID = {{WOS:000456088000001}},
DA = {{2021-10-27}},
}

@article{ WOS:000456088000005,
Author = {Lebedev, V and Lempitsky, V},
Title = {{Speeding-up convolutional neural networks: A survey}},
Journal = {{BULLETIN OF THE POLISH ACADEMY OF SCIENCES-TECHNICAL SCIENCES}},
Year = {{2018}},
Volume = {{66}},
Number = {{6}},
Month = {{DEC}},
Abstract = {{Convolutional neural networks (CNN) have become ubiquitous in computer
   vision as well as several other domains, but the sheer size of the
   modern CNNs means that for the majority of practical applications, a
   significant speed up and compression are often required. Speeding-up
   CNNs therefore have become a very active area of research with multiple
   diverse research directions pursued by many groups in academia and
   industry. In this short survey, we cover several research directions for
   speeding up CNNs that have become popular recently. Specifically, we
   cover approaches based on tensor decompositions, weight quantization,
   weight pruning, and teacher-student approaches. We also review CNN
   architectures designed for optimal speed and briefly consider automatic
   architecture search.}},
Publisher = {{POLSKA AKAD NAUK, POLISH ACAD SCI, DIV IV TECHNICAL SCIENCES PAS}},
Address = {{PL DEFILAD 1, WARSZAWA, 00-901, POLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Lebedev, V (Corresponding Author), Skolkovo Insitute Sci \& Technol, Moscow, Russia.
   Lebedev, V (Corresponding Author), Yandex, Moscow, Russia.
   Lebedev, V; Lempitsky, V, Skolkovo Insitute Sci \& Technol, Moscow, Russia.
   Lebedev, V, Yandex, Moscow, Russia.}},
DOI = {{10.24425/bpas.2018.125927}},
ISSN = {{0239-7528}},
EISSN = {{2300-1917}},
Keywords = {{convolutional neural networks; resource-efficient computation; algorithm
   optimization}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Multidisciplinary}},
Author-Email = {{vadim.lebedev@skoltech.ru}},
Funding-Acknowledgement = {{Ministry of Education and Science of the Russian FederationMinistry of
   Education and Science, Russian Federation {[}14.756.31.0001]}},
Funding-Text = {{This work is supported by the Ministry of Education and Science of the
   Russian Federation (grant 14.756.31.0001)}},
Number-of-Cited-References = {{85}},
Times-Cited = {{11}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{Bull. Pol. Acad. Sci.-Tech. Sci.}},
Doc-Delivery-Number = {{HH9VG}},
Unique-ID = {{WOS:000456088000005}},
DA = {{2021-10-27}},
}

@article{ WOS:000446683700013,
Author = {Kononenko, Daniil and Ganin, Yaroslav and Sungatullina, Diana and
   Lempitsky, Victor},
Title = {{Photorealistic Monocular Gaze Redirection Using Machine Learning}},
Journal = {{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}},
Year = {{2018}},
Volume = {{40}},
Number = {{11}},
Pages = {{2696-2710}},
Month = {{NOV}},
Abstract = {{We propose a general approach to the gaze redirection problem in images
   that utilizes machine learning. The idea is to learn to re-synthesize
   images by training on pairs of images with known disparities between
   gaze directions. We show that such learning-based re-synthesis can
   achieve convincing gaze redirection based on monocular input, and that
   the learned systems generalize well to people and imaging conditions
   unseen during training. We describe and compare three instantiations of
   our idea. The first system is based on efficient decision forest
   predictors and redirects the gaze by a fixed angle in real-time (on a
   single CPU), being particularly suitable for the videoconferencing gaze
   correction. The second system is based on a deep architecture and allows
   gaze redirection by a range of angles. The second system achieves higher
   photorealism, while being several times slower. The third system is
   based on real-time decision forests at test time, while using the
   supervision from a ``teacher{''} deep network during training. The third
   system approaches the quality of a teacher network in our experiments,
   and thus provides a highly realistic real-time monocular solution to the
   gaze correction problem. We present in-depth assessment and comparisons
   of the proposed systems based on quantitative measurements and a user
   study.}},
Publisher = {{IEEE COMPUTER SOC}},
Address = {{10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Kononenko, D (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow 143026, Russia.
   Kononenko, Daniil; Sungatullina, Diana; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow 143026, Russia.
   Ganin, Yaroslav, Univ Montreal, Montreal, PQ, Canada.}},
DOI = {{10.1109/TPAMI.2017.2737423}},
ISSN = {{0162-8828}},
EISSN = {{1939-3539}},
Keywords = {{Gaze redirection; machine learning; deep learning; random forest;
   weakly-supervised learning; image resynthesis}},
Keywords-Plus = {{EYE CONTACT; FORESTS}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{daniil.kononenko@skoltech.ru
   yaroslav.ganin@gmail.com
   d.sungatullina@skoltech.ru
   lempitsky@skoltech.ru}},
Number-of-Cited-References = {{50}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{IEEE Trans. Pattern Anal. Mach. Intell.}},
Doc-Delivery-Number = {{GW2AF}},
Unique-ID = {{WOS:000446683700013}},
DA = {{2021-10-27}},
}

@article{ WOS:000454184600003,
Author = {Vakhitov, Alexander and Kuzmin, Andrey and Lempitsky, Victor},
Title = {{Set2Model networks: Learning discriminatively to learn generative models}},
Journal = {{COMPUTER VISION AND IMAGE UNDERSTANDING}},
Year = {{2018}},
Volume = {{173}},
Pages = {{13-23}},
Month = {{AUG}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{We present a new ``learning-to-learn{''}-type approach that enables
   rapid learning of concepts from small to-medium sized training sets and
   is primarily designed for web-initialized image retrieval. At the core
   of our approach is a deep architecture (a Set2Model network) that maps
   sets of examples to simple generative probabilistic models such as
   Gaussians or mixtures of Gaussians in the space of high-dimensional
   descriptors. The parameters of the embedding into the descriptor space
   are trained in the end-to-end fashion in the meta-learning stage using a
   set of training learning problems. The main technical novelty of our
   approach is the derivation of the backprop process through the mixture
   model fitting, which makes the likelihood of the resulting models
   differentiable with respect to the positions of the input descriptors.
   While the meta-learning process for a Set2Model network is
   discriminative, a trained Set2Model network performs generative learning
   of generative models in the descriptor space, which facilitates learning
   in the cases when no negative examples are available, and whenever the
   concept being learned is polysemous or represented by noisy training
   sets. Among other experiments, we demonstrate that these properties
   allow Set2Model networks to pick visual concepts from the raw outputs of
   Internet image search engines better than a set of strong baselines. (C)
   2017 Elsevier Inc. All rights reserved.}},
Publisher = {{ACADEMIC PRESS INC ELSEVIER SCIENCE}},
Address = {{525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA}},
Type = {{Article; Proceedings Paper}},
Language = {{English}},
Affiliation = {{Vakhitov, A (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Vakhitov, Alexander; Kuzmin, Andrey; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1016/j.cviu.2017.08.001}},
ISSN = {{1077-3142}},
EISSN = {{1090-235X}},
Keywords = {{Learning-to-learn; Deep learning; Internet-based computer vision; Image
   retrieval; Gaussian mixture model; ImageNet}},
Keywords-Plus = {{IMAGE}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{a.vakhitov@skoltech.ru
   a.kuzmin@skoltech.ru
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Russian MES grant {[}RFMEFI61516X0003]}},
Funding-Text = {{Research was supported by the Russian MES grant RFMEFI61516X0003.}},
Number-of-Cited-References = {{37}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{7}},
Journal-ISO = {{Comput. Vis. Image Underst.}},
Doc-Delivery-Number = {{HF4DY}},
Unique-ID = {{WOS:000454184600003}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@article{ WOS:000431374900051,
Author = {Liotti, Enzo and Arteta, Carlos and Zisserman, Andrew and Lui, Andrew
   and Lempitsky, Victor and Grant, Patrick S.},
Title = {{Crystal nucleation in metallic alloys using x-ray radiography and
   machine learning}},
Journal = {{SCIENCE ADVANCES}},
Year = {{2018}},
Volume = {{4}},
Number = {{4}},
Month = {{APR}},
Abstract = {{The crystallization of solidifying Al-Cu alloys over a wide range of
   conditions was studied in situ by synchrotron x-ray radiography, and the
   data were analyzed using a computer vision algorithm trained using
   machine learning. The effect of cooling rate and solute concentration on
   nucleation undercooling, crystal formation rate, and crystal growth rate
   was measured automatically for thousands of separate crystals, which was
   impossible to achieve manually. Nucleation undercooling distributions
   confirmed the efficiency of extrinsic grain refiners and gave support to
   the widely assumed free growth model of heterogeneous nucleation. We
   show that crystallization occurred in temporal and spatial bursts
   associated with a solute-suppressed nucleation zone.}},
Publisher = {{AMER ASSOC ADVANCEMENT SCIENCE}},
Address = {{1200 NEW YORK AVE, NW, WASHINGTON, DC 20005 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Liotti, E (Corresponding Author), Univ Oxford, Dept Mat, Oxford OX1 3PH, England.
   Liotti, Enzo; Lui, Andrew; Grant, Patrick S., Univ Oxford, Dept Mat, Oxford OX1 3PH, England.
   Arteta, Carlos; Zisserman, Andrew, Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England.
   Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1126/sciadv.aar4004}},
Article-Number = {{eaar4004}},
ISSN = {{2375-2548}},
Keywords-Plus = {{IN-SITU; GRAIN-REFINEMENT; DENDRITE FRAGMENTATION; ALUMINUM-ALLOYS;
   REAL-TIME; DIRECTIONAL SOLIDIFICATION; EQUIAXED TRANSITION;
   PHASE-CHANGE; MICROSCOPY; GROWTH}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{enzo.liotti@materials.ox.ac.uk}},
ResearcherID-Numbers = {{Grant, Patrick/F-5169-2013
   }},
ORCID-Numbers = {{Grant, Patrick/0000-0002-7942-7837
   Liotti, Enzo/0000-0002-9267-5157
   Zisserman, Andrew/0000-0002-8945-8573}},
Funding-Acknowledgement = {{UK Engineering and Physical Sciences Research Council through the Future
   Manufacturing Hub in Liquid Metal EngineeringUK Research \& Innovation
   (UKRI)Engineering \& Physical Sciences Research Council (EPSRC)
   {[}EP/N007638/1]; UK Engineering and Physical Sciences Research Council
   through Programme Grant Seebibyte: Visual Search for the Era of Big Data
   {[}EP/M013774/1]}},
Funding-Text = {{We thank the UK Engineering and Physical Sciences Research Council for
   funding through the Future Manufacturing Hub in Liquid Metal Engineering
   (EP/N007638/1) and Programme Grant Seebibyte: Visual Search for the Era
   of Big Data (EP/M013774/1). This work was enabled by synchrotron
   beamtime at ESRF ID19 (experiment no. MA2035).}},
Number-of-Cited-References = {{71}},
Times-Cited = {{42}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{51}},
Journal-ISO = {{Sci. Adv.}},
Doc-Delivery-Number = {{GE6XR}},
Unique-ID = {{WOS:000431374900051}},
OA = {{Green Published, Green Submitted, gold}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000461852000069,
Author = {Burkov, Egor and Lempitsky, Victor},
Editor = {{Bengio, S and Wallach, H and Larochelle, H and Grauman, K and CesaBianchi, N and Garnett, R}},
Title = {{Deep Neural Networks with Box Convolutions}},
Booktitle = {{ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)}},
Series = {{Advances in Neural Information Processing Systems}},
Year = {{2018}},
Volume = {{31}},
Note = {{32nd Conference on Neural Information Processing Systems (NIPS),
   Montreal, CANADA, DEC 02-08, 2018}},
Abstract = {{Box filters computed using integral images have been part of the
   computer vision toolset for a long time. Here, we show that a
   convolutional layer that computes box filter responses in a sliding
   manner can be used within deep architectures, whereas the dimensions and
   the offsets of the sliding boxes in such a layer can be learned as a
   part of an end-to-end loss minimization. Crucially, the training process
   can make the size of the boxes in such a layer arbitrarily large without
   incurring extra computational cost and without the need to increase the
   number of learnable parameters. Due to its ability to integrate
   information over large boxes, the new layer facilitates long-range
   propagation of information and leads to the efficient increase of the
   receptive fields of network units. By incorporating the new layer into
   existing architectures for semantic segmentation, we are able to achieve
   both the increase in segmentation accuracy as well as the decrease in
   the computational cost and the number of learnable parameters.}},
Publisher = {{NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)}},
Address = {{10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Burkov, E (Corresponding Author), Samsung AI Ctr, Moscow, Russia.
   Burkov, E (Corresponding Author), Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.
   Burkov, Egor; Lempitsky, Victor, Samsung AI Ctr, Moscow, Russia.
   Burkov, Egor; Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.}},
ISSN = {{1049-5258}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Funding-Acknowledgement = {{Skolkovo Institute of Science and Technology; Ministry of Science of
   Russian Federation {[}14.756.31.0001]}},
Funding-Text = {{Most of the work was done when both authors were full-time with Skolkovo
   Institute of Science and Technology. The work was supported by the
   Ministry of Science of Russian Federation grant 14.756.31.0001.}},
Number-of-Cited-References = {{35}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM3CA}},
Unique-ID = {{WOS:000461852000069}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000454996700076,
Author = {Kononenko, Daniil and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Semi-Supervised Learning for Monocular Gaze Redirection}},
Booktitle = {{PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE \&
   GESTURE RECOGNITION (FG 2018)}},
Series = {{IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops}},
Year = {{2018}},
Pages = {{535-539}},
Note = {{13th IEEE International Conference on Automatic Face \& Gesture
   Recognition (FG), Xi an, PEOPLES R CHINA, MAY 15-19, 2018}},
Organization = {{IEEE Comp Soc; IEEE Biometr Council}},
Abstract = {{We present a new approach to monocular learning-based gaze redirection
   problem in images that is able to train on raw sequences of eye images
   with unknown gaze directions and a small amount of eye images, where the
   gaze direction is known. The proposed approach is based on a pair of
   deep networks, where the first encoder-like network maps eye images to a
   latent space, while the second network maps pairs of latent
   representations to warping fields implementing the transformation
   between the pair of the original images. In the proposed system, both
   networks are trained in an unsupervised manner, while the gaze-annotated
   images are only used to estimate displacements in the latent space that
   are characteristic to certain gaze redirections. Quantitative and
   qualitative evaluation suggests that such characteristic displacement
   vectors in the learned latent space can be learned from few examples and
   are transferable across different people and different imaging
   conditions.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Kononenko, D (Corresponding Author), Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.
   Kononenko, Daniil; Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.}},
DOI = {{10.1109/FG.2018.00086}},
ISSN = {{2326-5396}},
ISBN = {{978-1-5386-2335-0}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Funding-Acknowledgement = {{Russian Federation MES grant {[}14.756.31.0001]}},
Funding-Text = {{This study was supported by the Russian Federation MES grant
   14.756.31.0001.}},
Number-of-Cited-References = {{13}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BL7GJ}},
Unique-ID = {{WOS:000454996700076}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000518410600231,
Author = {Lempitsky, Victor and Vakhitov, Alexander and Starostin, Andrew},
Editor = {{Kiyokawa, K and Steinicke, F and Thomas, B and Welch, G}},
Title = {{Carpet VR: the Magic Carpet Meets the Magic Mirror}},
Booktitle = {{25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR)}},
Year = {{2018}},
Pages = {{833}},
Note = {{25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE
   VR), Reutlingen, GERMANY, MAR 18-22, 2018}},
Organization = {{IEEE; IEEE Comp Soc; IEEE Comp Soc Visualizat \& Graph Tech Comm; VICON;
   Digital Project; ART; Haption; MiddleVR; VR ON; VISCON; BARCO; WorldViz;
   Disney Res; Chinese Acad Sci, Comp Network Informat Ctr; KUKA}},
Abstract = {{We present CarpetVR - a new system for marker-based positional tracking
   suitable for mobile VR (Figure 1). The system utilizes all sensors
   present on a modern smartphone (a camera, a gyroscope, and an
   accelerometer) and does not require any additional sensors. CarpetVR
   uses a single floor marker that we call the magic carpet. CarpetVR
   augments a standard mobile VR setup with a slanted mirror that can be
   attached either to the smartphone or to the head mount in front of the
   smartphone camera. As the person walks over the marker, the smartphone
   camera is able to see the marker thanks to the reflection in the mirror.
   Our tracking engine then uses a computer vision module to detect the
   marker and to estimate the smartphone position with respect to the
   marker at 40 frames per second. This estimate is integrated with high
   framerate signals from the gyroscope and the accelerometer. The
   resulting estimates of the position and the orientation are then used to
   render the virtual world. Our sensor fusion algorithm ensures
   minimal-latency tracking with very little jitter.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Lempitsky, V (Corresponding Author), Skolkovo Inst Sci \& Technol, CarpetVR, Moscow, Russia.
   Lempitsky, Victor; Vakhitov, Alexander; Starostin, Andrew, Skolkovo Inst Sci \& Technol, CarpetVR, Moscow, Russia.}},
ISBN = {{978-1-5386-3365-6}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering}},
Author-Email = {{lempitsky@skoltech.ru
   a.vakhitov@skoltech.ru
   andrew.starostin@skoltech.ru}},
Number-of-Cited-References = {{0}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BO5NH}},
Unique-ID = {{WOS:000518410600231}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000594219200036,
Author = {Sungatullina, Diana and Zakharov, Egor and Ulyanov, Dmitry and
   Lempitsky, Victor},
Editor = {{Ferrari, V and Hebert, M and Sminchisescu, C and Weiss, Y}},
Title = {{Image Manipulation with Perceptual Discriminators}},
Booktitle = {{COMPUTER VISION - ECCV 2018, PT VI}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2018}},
Volume = {{11210}},
Pages = {{587-602}},
Note = {{15th European Conference on Computer Vision (ECCV), Munich, GERMANY, SEP
   08-14, 2018}},
Abstract = {{Systems that perform image manipulation using deep convolutional
   networks have achieved remarkable realism. Perceptual losses and losses
   based on adversarial discriminators are the two main classes of learning
   objectives behind these advances. In this work, we show how these two
   ideas can be combined in a principled and non-additive manner for
   unaligned image translation tasks. This is accomplished through a
   special architecture of the discriminator network inside generative
   adversarial learning framework. The new architecture, that we call a
   perceptual discriminator, embeds the convolutional parts of a
   pre-trained deep classification network inside the discriminator
   network. The resulting architecture can be trained on unaligned image
   datasets, while benefiting from the robustness and efficiency of
   perceptual losses. We demonstrate the merits of the new architecture in
   a series of qualitative and quantitative comparisons with baseline
   approaches and state-of-the-art frameworks for unaligned image
   translation.}},
Publisher = {{SPRINGER INTERNATIONAL PUBLISHING AG}},
Address = {{GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Sungatullina, D (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Sungatullina, Diana; Zakharov, Egor; Ulyanov, Dmitry; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1007/978-3-030-01231-1\_36}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-030-01231-1; 978-3-030-01230-4}},
Keywords = {{Image translation; Image editing; Perceptual loss; Generative
   adversarial networks}},
Research-Areas = {{Computer Science; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Imaging Science \&
   Photographic Technology}},
Author-Email = {{d.sungatullina@skoltech.ru
   egor.zakharov@skoltech.ru
   dmitry.ulyanov@skoltech.ru
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Ministry of Education and Science of the Russian FederationMinistry of
   Education and Science, Russian Federation {[}14.756.31.0001]}},
Funding-Text = {{This work has been supported by the Ministry of Education and Science of
   the Russian Federation (grant 14.756.31.0001).}},
Number-of-Cited-References = {{41}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BQ4SA}},
Unique-ID = {{WOS:000594219200036}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000457843609064,
Author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Deep Image Prior}},
Booktitle = {{2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2018}},
Pages = {{9446-9454}},
Note = {{31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Salt Lake City, UT, JUN 18-23, 2018}},
Organization = {{IEEE; CVF; IEEE Comp Soc}},
Abstract = {{Deep convolutional networks have become a popular tool for image
   generation and restoration. Generally, their excellent performance is
   imputed to their ability to learn realistic image priors from a large
   number of example images. In this paper, we show that, on the contrary,
   the structure of a generator network is sufficient to capture a great
   deal of low-level image statistics prior to any learning. In order to do
   so, we show that a randomly-initialized neural network can be used as a
   handcrafted prior with excellent results in standard inverse problems
   such as denoising, super-resolution, and inpainting. Furthermore, the
   same prior can be used to invert deep neural representations to diagnose
   them, and to restore images based on flash-no flash input pairs.
   Apart from its diverse applications, our approach highlights the
   inductive bias captured by standard generator network architectures. It
   also bridges the gap between two very popular families of image
   restoration methods: learning-based methods using deep convolutional
   networks and learning-free methods based on handcrafted image priors
   such as self-similarity.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Ulyanov, D (Corresponding Author), Yandex, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Ulyanov, Dmitry, Yandex, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Vedaldi, Andrea, Univ Oxford, Oxford, England.
   Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.}},
DOI = {{10.1109/CVPR.2018.00984}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-6420-9}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{dmitry.ulyanov@skoltech.ru
   vedaldi@robots.ox.ac.uk
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Ministry of Education and Science of the Russian FederationMinistry of
   Education and Science, Russian Federation {[}14.756.31.0001];
   ERCEuropean Research Council (ERC)European Commission {[}677195-IDIU]}},
Funding-Text = {{DU and VL are supported by the Ministry of Education and Science of the
   Russian Federation (grant 14.756.31.0001) and AV is supported by ERC
   677195-IDIU.}},
Number-of-Cited-References = {{33}},
Times-Cited = {{300}},
Usage-Count-Last-180-days = {{9}},
Usage-Count-Since-2013 = {{35}},
Doc-Delivery-Number = {{BL9NZ}},
Unique-ID = {{WOS:000457843609064}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000485488901040,
Author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
Book-Group-Author = {{AAAI}},
Title = {{It Takes (Only) Two: Adversarial Generator- Encoder Networks}},
Booktitle = {{THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH
   INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH
   AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE}},
Year = {{2018}},
Pages = {{1250-1257}},
Note = {{32nd AAAI Conference on Artificial Intelligence / 30th Innovative
   Applications of Artificial Intelligence Conference / 8th AAAI Symposium
   on Educational Advances in Artificial Intelligence, New Orleans, LA, FEB
   02-07, 2018}},
Organization = {{AAAI}},
Abstract = {{We present a new autoencoder-type architecture that is trainable in an
   unsupervised mode, sustains both generation and inference, and has the
   quality of conditional and unconditional samples boosted by adversarial
   learning. Unlike previous hybrids of autoencoders and adversarial
   networks, the adversarial game in our approach is set up directly
   between the encoder and the generator, and no external mappings are
   trained in the process of learning. The game objective compares the
   divergences of each of the real and the generated data distributions
   with the prior distribution in the latent space. We show that direct
   generator-vs-encoder game leads to a tight coupling of the two
   components, resulting in samples and reconstructions of a comparable
   quality to some recently-proposed more complex architectures.}},
Publisher = {{ASSOC ADVANCEMENT ARTIFICIAL INTELLIGENCE}},
Address = {{2275 E BAYSHORE RD, STE 160, PALO ALTO, CA 94303 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Ulyanov, D (Corresponding Author), Yandex, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Ulyanov, Dmitry, Yandex, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Vedaldi, Andrea, Univ Oxford, Oxford, England.
   Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
ISBN = {{978-1-57735-800-8}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic}},
Author-Email = {{dmitry.ulyanov@skoltech.ru
   vedaldi@robots.ox.ac.uk
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{ERCEuropean Research Council (ERC)European Commission {[}677195-IDIU]; 
   {[}14.756.31.0001]}},
Funding-Text = {{This study was supported by grants 14.756.31.0001 and ERC 677195-IDIU.}},
Number-of-Cited-References = {{28}},
Times-Cited = {{11}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BN6JJ}},
Unique-ID = {{WOS:000485488901040}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000594226800040,
Author = {Vakhitov, Alexander and Lempitsky, Victor and Zheng, Yinqiang},
Editor = {{Ferrari, V and Hebert, M and Sminchisescu, C and Weiss, Y}},
Title = {{Stereo Relative Pose from Line and Point Feature Triplets}},
Booktitle = {{COMPUTER VISION - ECCV 2018, PT VIII}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2018}},
Volume = {{11212}},
Pages = {{662-677}},
Note = {{15th European Conference on Computer Vision (ECCV), Munich, GERMANY, SEP
   08-14, 2018}},
Abstract = {{Stereo relative pose problem lies at the core of stereo visual odometry
   systems that are used in many applications. In this work we present two
   minimal solvers for the stereo relative pose. We specifically consider
   the case when a minimal set consists of three point or line features and
   each of them has three known projections on two stereo cameras. We
   validate the importance of this formulation for practical purposes in
   our experiments with motion estimation. We then present a complete
   classification of minimal cases with three point or line correspondences
   each having three projections, and present two new solvers that can
   handle all such cases. We demonstrate a considerable effect from the
   integration of the new solvers into a visual SLAM system.}},
Publisher = {{SPRINGER INTERNATIONAL PUBLISHING AG}},
Address = {{GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Vakhitov, A (Corresponding Author), Skoltech, Moscow Nobelya Ulitsa 3, Moscow 121207, Russia.
   Vakhitov, Alexander; Lempitsky, Victor, Skoltech, Moscow Nobelya Ulitsa 3, Moscow 121207, Russia.
   Zheng, Yinqiang, NII, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.}},
DOI = {{10.1007/978-3-030-01237-3\_40}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-030-01237-3; 978-3-030-01236-6}},
Keywords = {{Minimal solver; Stereo visual odometry; Generalized camera; Relative
   pose; Line features}},
Keywords-Plus = {{STRUCTURE-FROM-MOTION; EFFICIENT}},
Research-Areas = {{Computer Science; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Imaging Science \&
   Photographic Technology}},
Author-Email = {{a.vakhitov@skoltech.ru
   lempitsky@skoltech.ru
   yqzheng@nii.ac.jp}},
Funding-Acknowledgement = {{Russian MES grant {[}RFMEFI61516X0003]; NII MOU/Non-MOU International
   Exchange Program}},
Funding-Text = {{The work is funded by the Russian MES grant RFMEFI61516X0003; a part of
   this work was finished when Alexander Vakhitov was visiting the National
   Institute of Informatics (NII), Japan, funded by the NII MOU/Non-MOU
   International Exchange Program.}},
Number-of-Cited-References = {{31}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BQ4SC}},
Unique-ID = {{WOS:000594226800040}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000425498404102,
Author = {Babenko, Artem and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{AnnArbor: Approximate Nearest Neighbors Using Arborescence Coding}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2017}},
Pages = {{4895-4903}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{To compress large datasets of high-dimensional descriptors, modern
   quantization schemes learn multiple code-books and then represent
   individual descriptors as combinations of codewords. Once the codebooks
   are learned, these schemes encode descriptors independently. In contrast
   to that, we present a new coding scheme that arranges dataset
   descriptors into a set of arborescence graphs, and then encodes non-root
   descriptors by quantizing their displacements with respect to their
   parent nodes. By optimizing the structure of arborescences, our coding
   scheme can decrease the quantization error considerably, while incurring
   only minimal overhead on the memory footprint and the speed of nearest
   neighbor search in the compressed dataset compared to the independent
   quantization. The advantage of the proposed scheme is demonstrated in a
   series of experiments with datasets of SIFT and deep descriptors.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Babenko, A (Corresponding Author), Yandex, Moscow, Russia.
   Babenko, A (Corresponding Author), Natl Res Univ, Higher Sch Econ, Moscow, Russia.
   Babenko, Artem, Yandex, Moscow, Russia.
   Babenko, Artem, Natl Res Univ, Higher Sch Econ, Moscow, Russia.
   Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.}},
DOI = {{10.1109/ICCV.2017.523}},
ISSN = {{1550-5499}},
ISBN = {{978-1-5386-1032-9}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{artem.babenko@phystech.edu
   lempitsky@skoltech.ru}},
ResearcherID-Numbers = {{Babenko, Artem/M-3540-2016}},
ORCID-Numbers = {{Babenko, Artem/0000-0002-1830-8252}},
Funding-Acknowledgement = {{MES RF {[}14.756.31.0001]}},
Funding-Text = {{VL is supported by the MES RF grant 14.756.31.0001.}},
Number-of-Cited-References = {{22}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BJ4TW}},
Unique-ID = {{WOS:000425498404102}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000418371406044,
Author = {Babenko, Artem and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Product Split Trees}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{6316-6324}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{In this work, we introduce a new kind of spatial partition trees for
   efficient nearest-neighbor search. Our approach first identifies a set
   of useful data splitting directions, and then learns a codebook that can
   be used to encode such directions. We use the product-quantization idea
   in order to make the effective codebook large, the evaluation of scalar
   products between the query and the encoded splitting direction very
   fast, and the encoding itself compact. As a result, the proposed data
   srtucture (Product Split tree) achieves compact clustering of data
   points, while keeping the traversal very efficient. In the
   nearest-neighbor search experiments on high-dimensional data, product
   split trees achieved state-of-the-art performance, demonstrating better
   speed-accuracy tradeoff than other spatial partition trees.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Babenko, A (Corresponding Author), Yandex, Moscow, Russia.
   Babenko, A (Corresponding Author), Natl Res Univ, Higher Sch Econ, Moscow, Russia.
   Babenko, Artem, Yandex, Moscow, Russia.
   Babenko, Artem, Natl Res Univ, Higher Sch Econ, Moscow, Russia.
   Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Moscow, Russia.}},
DOI = {{10.1109/CVPR.2017.669}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic}},
Author-Email = {{artem.babenko@phystech.edu
   lempitsky@skoltech.ru}},
ResearcherID-Numbers = {{Babenko, Artem/M-3540-2016}},
ORCID-Numbers = {{Babenko, Artem/0000-0002-1830-8252}},
Number-of-Cited-References = {{22}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BJ1XG}},
Unique-ID = {{WOS:000418371406044}},
DA = {{2021-10-27}},
}

@incollection{ WOS:000449836400011,
Author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain,
   Pascal and Larochelle, Hugo and Laviolette, Francois and Marchand, Mario
   and Lempitsky, Victor},
Editor = {{Csurka, G}},
Title = {{Domain-Adversarial Training of Neural Networks}},
Booktitle = {{DOMAIN ADAPTATION IN COMPUTER VISION APPLICATIONS}},
Series = {{Advances in Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{189-209}},
Abstract = {{We introduce a representation learning approach for domain adaptation,
   in which data at training and test time come from similar but different
   distributions. Our approach is directly inspired by the theory on domain
   adaptation suggesting that, for effective domain transfer to be
   achieved, predictions must be made based on features that cannot
   discriminate between the training (source) and test (target) domains.
   The approach implements this idea in the context of neural network
   architectures that are trained on labeled data from the source domain
   and unlabeled data from the target domain (no labeled target-domain data
   is necessary). As the training progresses, the approach promotes the
   emergence of features that are (i) discriminative for the main learning
   task on the source domain and (ii) indiscriminate with respect to the
   shift between the domains. We show that this adaptation behavior can be
   achieved in almost any feed-forward model by augmenting it with few
   standard layers and a new Gradient Reversal Layer. The resulting
   augmented architecture can be trained using standard backpropagation,
   and can thus be implemented with little effort using any of the deep
   learning packages. We demonstrate the success of our approach for image
   classification, where state-of-the-art domain adaptation performance on
   standard benchmarks is achieved. We also validate the approach for
   descriptor learning task in the context of person re-identification
   application.}},
Publisher = {{SPRINGER-VERLAG LONDON LTD}},
Address = {{SWEETAPPLE HOUSE CATTESHALL RD FARNCOMBE, GODALMING GU7 1NH, SURREY,
   ENGLAND}},
Type = {{Article; Book Chapter}},
Language = {{English}},
Affiliation = {{Ganin, Y (Corresponding Author), Skolkovo Inst Sci \& Technol, Skolkovo, Moscow Region, Russia.
   Ganin, Yaroslav; Ustinova, Evgeniya; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Skolkovo, Moscow Region, Russia.
   Ajakan, Hana; Laviolette, Francois; Marchand, Mario, Univ Laval, Dept Informat \& Genie Logiciel, Quebec City, PQ, Canada.
   Germain, Pascal, Ecole Normale Super, INRIA Paris, Paris, France.
   Larochelle, Hugo, Twitter, Cambridge, MA USA.
   Larochelle, Hugo, Univ Sherbrooke, Quebec City, PQ, Canada.}},
DOI = {{10.1007/978-3-319-58347-1\_10}},
ISSN = {{2191-6586}},
ISBN = {{978-3-319-58347-1; 978-3-319-58346-4}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{ganin@skoltech.ru
   evgeniya.ustinova@skoltech.ru
   hana.ajakan.1@ulaval.ca
   pascal.germain@inria.fr
   hugo.larochelle@usherbrooke.ca
   francois.laviolette@ift.ulaval.ca
   mario.marchand@ift.ulaval.ca
   lempitsky@skoltech.ru}},
Number-of-Cited-References = {{0}},
Times-Cited = {{38}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BL3LD}},
Unique-ID = {{WOS:000449836400011}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000425498400090,
Author = {Klokov, Roman and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point
   Cloud Models}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)}},
Series = {{IEEE International Conference on Computer Vision}},
Year = {{2017}},
Pages = {{863-872}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{We present a new deep learning architecture (called Kd-network) that is
   designed for 3D model recognition tasks and works with unstructured
   point clouds. The new architecture performs multiplicative
   transformations and shares parameters of these transformations according
   to the subdivisions of the point clouds imposed onto them by kd-trees.
   Unlike the currently dominant convolutional architectures that usually
   require rasterization on uniform two-dimensional or three-dimensional
   grids, Kd-networks do not rely on such grids in any way and therefore
   avoid poor scaling behavior. In a series of experiments with popular
   shape recognition benchmarks, Kd-networks demonstrate competitive
   performance in a number of shape recognition tasks such as shape
   classification, shape retrieval and shape part segmentation.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Klokov, R (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Klokov, Roman; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1109/ICCV.2017.99}},
ISSN = {{1550-5499}},
ISBN = {{978-1-5386-1032-9}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{roman.klokov@skoltech.ru
   lempitsky@skoltech.ru}},
ORCID-Numbers = {{Klokov, Roman/0000-0001-9592-7009}},
Funding-Acknowledgement = {{Russian MES grant {[}RFMEFI61516X0003]}},
Funding-Text = {{this work is supported by the Russian MES grant RFMEFI61516X0003.}},
Number-of-Cited-References = {{37}},
Times-Cited = {{264}},
Usage-Count-Last-180-days = {{9}},
Usage-Count-Since-2013 = {{35}},
Doc-Delivery-Number = {{BJ4TW}},
Unique-ID = {{WOS:000425498400090}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000425458700078,
Author = {Kuzmin, Andrey and Mikushin, Dmitry and Lempitsky, Victor},
Editor = {{Ueda, N and Watanabe, S and Matsui, T and Chien, JT and Larsen, J}},
Title = {{END-TO-END LEARNING OF COST-VOLUME AGGREGATION FOR REAL-TIME DENSE}},
Booktitle = {{2017 IEEE 27TH INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL
   PROCESSING}},
Series = {{IEEE International Workshop on Machine Learning for Signal Processing}},
Year = {{2017}},
Note = {{27th IEEE International Workshop on Machine Learning for Signal
   Processing (MLSP), Int House Japan, Tokyo, JAPAN, SEP 25-28, 2017}},
Organization = {{IEEE; IEEE Signal Proc Soc, Machine Learning Signal Proc Tech Comm}},
Abstract = {{We present a new deep learning-based approach for dense stereo matching.
   Compared to previous works, our approach does not use deep learning of
   pixel appearance descriptors, employing very fast classical matching
   scores instead. At the same time, our approach uses a deep convolutional
   network to predict the local parameters of cost volume aggregation
   process, which in this paper we implement using differentiable domain
   transform. By treating such transform as a recurrent neural network, we
   are able to train our whole system that includes cost volume
   computation, cost-volume aggregation ( smoothing), and winner-takes-all
   disparity selection end-to-end. The resulting method is highly efficient
   at test time, while achieving good matching accuracy. On the KITTI 2012
   and KITTI 2015 benchmark, it achieves a result of 5.08\% and 6.34\%
   error rate respectively while running at 29 frames per second rate on a
   modern GPU.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Kuzmin, A (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Kuzmin, Andrey; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Mikushin, Dmitry, Appl Parallel Comp LLC, Zurich, Switzerland.}},
ISSN = {{2161-0363}},
ISBN = {{978-1-5090-6341-3}},
Keywords = {{stereo matching; cost-volume aggregation; edge-preserving filtering;
   convolutional neural network; recurrent neural network}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Electrical \& Electronic}},
Funding-Acknowledgement = {{Ministry of Education and Science of the Russian FederationMinistry of
   Education and Science, Russian Federation {[}14.756.31.0001]}},
Funding-Text = {{This study was supported by the Ministry of Education and Science of the
   Russian Federation (grant 14.756.31.0001)}},
Number-of-Cited-References = {{28}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BJ4SM}},
Unique-ID = {{WOS:000425458700078}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000418371404021,
Author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Improved Texture Networks: Maximizing Quality and Diversity in
   Feed-forward Stylization and Texture Synthesis}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{4105-4113}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{The recent work of Gatys et al., who characterized the style of an image
   by the statistics of convolutional neural network filters, ignited a
   renewed interest in the texture generation and image stylization
   problems. While their image generation technique uses a slow
   optimization process, recently several authors have proposed to learn
   generator neural networks that can produce similar outputs in one quick
   forward pass. While generator networks are promising, they are still
   inferior in visual quality and diversity compared to
   generation-by-optimization. In this work, we advance them in two
   significant ways. First, we introduce an instance normalization module
   to replace batch normalization with significant improvements to the
   quality of image stylization. Second, we improve diversity by
   introducing a new learning formulation that encourages generators to
   sample unbiasedly from the Julesz texture ensemble, which is the
   equivalence class of all images characterized by certain filter
   responses. Together, these two improvements take feed forward texture
   synthesis and image stylization much closer to the quality of
   generation- via-optimization, while retaining the speed advantage.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Ulyanov, D (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Ulyanov, D (Corresponding Author), Yandex, Moscow, Russia.
   Ulyanov, Dmitry; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Ulyanov, Dmitry, Yandex, Moscow, Russia.
   Vedaldi, Andrea, Univ Oxford, Oxford, England.}},
DOI = {{10.1109/CVPR.2017.437}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic}},
Author-Email = {{dmitry.ulyanov@skoltech.ru
   vedaldi@robots.ox.ac.uk
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Ministry of Education and Science of the Russian FederationMinistry of
   Education and Science, Russian Federation {[}14.756.31.0001]}},
Funding-Text = {{VL was supported by the Ministry of Education and Science of the Russian
   Federation (grant 14.756.31.0001).}},
Number-of-Cited-References = {{21}},
Times-Cited = {{66}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Doc-Delivery-Number = {{BJ1XG}},
Unique-ID = {{WOS:000418371404021}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000426203700002,
Author = {Ustinova, Evgeniya and Ganin, Yaroslav and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Multi-region Bilinear Convolutional Neural Networks for Person
   Re-Identification}},
Booktitle = {{2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL
   BASED SURVEILLANCE (AVSS)}},
Year = {{2017}},
Note = {{14th IEEE International Conference on Advanced Video and Signal Based
   Surveillance (AVSS), Lecce, ITALY, AUG 29-SEP 01, 2017}},
Organization = {{IEEE}},
Abstract = {{In this work we propose a new architecture for person re identification.
   As the task of re-identification is inherently associated with embedding
   learning and non-rigid appearance description, our architecture is based
   on the deep bilinear convolutional network (Bilinear-CNN) that has been
   proposed recently for fine-grained classification of highly non-rigid
   objects. While the last stages of the original Bilinear-CNN architecture
   completely removes the geometric information from consideration by
   performing orderless pooling, we observe that a better embedding can be
   learned by performing bilinear pooling in a more local way, where each
   pooling is confined to a predefined region. Our architecture thus
   represents a compromise between traditional convolutional networks and
   bilinear CNNs and strikes a balance between rigid matching and
   completely ignoring spatial information.
   We perform the experimental validation of the new architecture on the
   three popular benchmark datasets (Market 1501, CUHKOI, CUHK03),
   comparing it to baselines that include Bilinear-CNN as well as prior
   art. The new architecture outperforms the baseline on all three
   datasets, while performing better than state-of-the-art on two out of
   three. The code and the pretrained models of the approach will be made
   available at the time of publication.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Ustinova, E (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Ustinova, Evgeniya; Ganin, Yaroslav; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Ganin, Yaroslav, Montreal Inst Learning Algorithms, Montreal, PQ, Canada.}},
ISBN = {{978-1-5386-2939-0}},
Research-Areas = {{Computer Science; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Hardware \&
   Architecture; Imaging Science \& Photographic Technology}},
Author-Email = {{evgeniya.ustinova@skolkovotech.ru
   yaroslay.ganin@gmail.com
   lempitsky@skoltech.ru}},
Number-of-Cited-References = {{28}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BJ5QZ}},
Unique-ID = {{WOS:000426203700002}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000425239600043,
Author = {Vakhitov, Alexander and Kuzmin, Andrey and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Set2Model Networks: Learning Discriminatively To Learn Generative Models}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW
   2017)}},
Series = {{IEEE International Conference on Computer Vision Workshops}},
Year = {{2017}},
Pages = {{357-366}},
Note = {{16th IEEE International Conference on Computer Vision (ICCV), Venice,
   ITALY, OCT 22-29, 2017}},
Organization = {{IEEE; IEEE Comp Soc}},
Abstract = {{We present a new ``learning-to-learn{''}-type approach for
   small-to-medium sized training sets. At the core lies a deep
   architecture (a Set2Model network) that maps sets of examples to simple
   generative probabilistic models such as Gaussians or mixtures of
   Gaussians in the space of high-dimensional descriptors. The parameters
   of the embedding into the descriptor space are discriminatively trained
   in the end-to-end fashion. The main technical novelty of our approach is
   the derivation of the backprop process through the mixture model
   fitting. A trained Set2Model network facilitates learning in the cases
   when no negative examples are available, and whenever the concept being
   learned is polysemous or represented by noisy training sets. Among other
   experiments, we demonstrate that these properties allow Set2Model
   networks to pick visual concepts from the raw outputs of Internet image
   search engines better than a set of strong baselines.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Vakhitov, A (Corresponding Author), Skolkovo Inst Sci \& Technol, Moscow, Russia.
   Vakhitov, Alexander; Kuzmin, Andrey; Lempitsky, Victor, Skolkovo Inst Sci \& Technol, Moscow, Russia.}},
DOI = {{10.1109/ICCVW.2017.50}},
ISSN = {{2473-9936}},
ISBN = {{978-1-5386-1034-3}},
Keywords-Plus = {{CLASSIFICATION; IMAGE}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{a.vakhitov@skoltech.ru
   a.kuzmin@skolkovotech.ru
   lempitsky@skoltech.ru}},
Number-of-Cited-References = {{33}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BJ4OB}},
Unique-ID = {{WOS:000425239600043}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}

@inproceedings{ WOS:000418371404089,
Author = {Yurchenko, Victor and Lempitsky, Victor},
Book-Group-Author = {{IEEE}},
Title = {{Parsing Images of Overlapping Organisms with Deep Singling-Out Networks}},
Booktitle = {{30TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR
   2017)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2017}},
Pages = {{4752-4760}},
Note = {{30th IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR), Honolulu, HI, JUL 21-26, 2017}},
Organization = {{IEEE; IEEE Comp Soc; CVF}},
Abstract = {{This work is motivated by the mostly unsolved task of parsing biological
   images with multiple overlapping articulated model organisms (such as
   worms or larvae). We present a general approach that separates the two
   main challenges associated with such data, individual object shape
   estimation and object groups disentangling. At the core of the approach
   is a deep feed-forward singling-out network (SON) that is trained to map
   each local patch to a vectorial descriptor that is sensitive to the
   characteristics (e.g. shape) of a central object, while being invariant
   to the variability of all other surrounding elements. Given a SON, a
   local image patch can be matched to a gallery of isolated elements using
   their SON-descriptors, thus producing a hypothesis about the shape of
   the central element in that patch. The image-level optimization based on
   integer programming can then pick a subset of the hypotheses to explain
   (parse) the whole image and disentangle groups of organisms.
   While sharing many similarities with existing
   ``analysis-by-synthesis{''} approaches, our method avoids the need for
   stochastic search in the high-dimensional configuration space and
   numerous rendering operations at test-time. We show that our approach
   can parse microscopy images of three popular model organisms (the C.
   Elegans roundworms, the Drosophila larvae, and the E. Coli bacteria)
   even under significant crowding and overlaps between organisms. We
   speculate that the overall approach is applicable to a wider class of
   image parsing problems concerned with crowded articulated objects, for
   which rendering training images is possible.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Yurchenko, V (Corresponding Author), Skolkovo Inst Sci \& Technol Skoltech, Skolkovo, Moscow, Russia.
   Yurchenko, V (Corresponding Author), Yandex, Moscow, Russia.
   Yurchenko, Victor; Lempitsky, Victor, Skolkovo Inst Sci \& Technol Skoltech, Skolkovo, Moscow, Russia.
   Yurchenko, Victor, Yandex, Moscow, Russia.}},
DOI = {{10.1109/CVPR.2017.505}},
ISSN = {{1063-6919}},
ISBN = {{978-1-5386-0457-1}},
Keywords-Plus = {{INSTANCES; MULTIPLE}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic}},
Author-Email = {{victor.yurchenko@skoltech.ru
   lempitsky@skoltech.ru}},
Funding-Acknowledgement = {{Skoltech NGP Program (Skoltech-MIT joint project)}},
Funding-Text = {{This work was supported by Skoltech NGP Program (Skoltech-MIT joint
   project).}},
Number-of-Cited-References = {{23}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BJ1XG}},
Unique-ID = {{WOS:000418371404089}},
OA = {{Green Submitted}},
DA = {{2021-10-27}},
}
